# nlp-project
Code and data for the paper:
Abstract here:

## Data (training and test)
### Training data
In the training of the NMT model, we utilized the dataset:EN to-ZH ECCParaCorp and we used words/phrases extracted from the dataset:
In the training of LLM, we utilized the same dataset to generate prompts.
### Test data

## Fine-tuning opus-mt-en-zh
In the first phase, 
## Fine-tuning Qwen2.5 1.5B
Prompts are created using the 'create_prompt'function
## Inference

### Tokenizers
* **1.5B Qwen2.5 model**

### Translation

## Evaluation
Evaluation was done based on BLEU and human evaluation.
