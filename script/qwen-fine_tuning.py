# -*- coding: utf-8 -*-
"""Copy of llm_final_github.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GasKc2RomHZs8sc984J6clRla2mOp8J2

## 1 Packages
"""

pip install pandas datasets transformers evaluate torch tqdm

pip install datasets

pip install sacrebleu scikit-learn numpy

# import packages
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
import sacrebleu
from evaluate import load
from google.colab import files
from google.colab import drive
from sklearn.model_selection import train_test_split
import torch
import re
import random
import numpy as np
from tqdm import tqdm
from torch.optim import AdamW
from torch.optim.lr_scheduler import LambdaLR

"""## 2 Functions"""

# Function: create prompt
def construct_prompt(source, one_shot=True):

    # Part 1: Translate the sentence
    part_1 = f"Translate the following English medical terminology into Chinese: {source}\n"
    #part_2 = f"Provide only the translation, without any additional context."

    # Part 2: Add dictionary if words are found in word&phrase-type data
    #en_term = []
    #zh_term = []
    #for term, chinese_translation in medical_dict.items():
        #if term in source:
          #en_term.append(term)
          #zh_term.append(chinese_translation)

    #if en_term:
        #dictionary_prompt = "; ".join([f"In this context, the word '{word}' means {meanings}" for word, meanings in zip(en_term, zh_term)])
        #part_2 = f"{dictionary_prompt}.\n"
    #else:
        #part_2 = f""

    # Part 3: Asking for the full translation
    part_3 = f"The full translation to Chinese is:"

    # Combine all parts for the current instance
    current_instance = f"{part_1}{part_3}"

    if one_shot:
        # Randomly pick one demonstration
        random_demonstration = random.sample(demonstrations, 1)
        one_shot_prompt = f"Here is one example of your task.\n{random_demonstration[0]}\n\nComplete the task below."

        # Add one-shot demonstration to the current instance
        full_prompt = f"{one_shot_prompt}\n\n{current_instance}"
    else:
        full_prompt = current_instance

    return full_prompt

# Function: tokenize and form training dataset
def tokenize(examples):
    model_inputs = tokenizer(
        examples["source"],
        max_length=128,
        return_tensors="pt",
        truncation=True,
        padding="max_length"  # Ensure all sequences are padded to the same length
    )

    labels = tokenizer(
        examples["target"],
        max_length=128,
        return_tensors="pt",
        truncation=True,
        padding="max_length"
    )
    model_inputs["labels"] = labels["input_ids"]  # Add tokenized target texts as labels
    return model_inputs

# Function:extract translated text for BLEU calculation
def extract_translation(text):

    # Extract text after the second occurrence
    pattern = r"(?:The full translation to Chinese is:.*?){2}(.*)"
    match = re.search(pattern, text, re.DOTALL)

    if match:
        translation = match.group(1).strip()
    else:
        translation = "Translation not found!"

    return translation

# Evaluation: BLEU
metric = load("sacrebleu")

def compute_bleu_and_save(model, tokenizer, dataset, output_file="bleu_scores.txt"):
    """
    Compute BLEU score for a given model, tokenizer, and dataset,
    store detailed results in a text file and return both average BLEU score and batch BLEU score.
    """
    # Initialize the device and move model to appropriate device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    #tokenizer.pad_token_id = tokenizer.eos_token_id

    source_texts = dataset["source"]
    target_texts = dataset["target"]

    # Initialize variables to track BLEU scores and detailed results
    bleu_scores = []
    translated_texts = []  # Store all translations for batch BLEU
    references = []  # Store all references for batch BLEU

    # Open a text file to save the results
    with open(output_file, mode='w', encoding='utf-8') as file:
        # Write the header row
        file.write("Reference Sentence | Target Sentence | Translated Sentence | BLEU Score\n")
        file.write("=" * 100 + "\n")

        # Generate translations and compute BLEU for each sentence
        for i in tqdm(range(len(source_texts)), desc="Evaluating BLEU"):
            text = source_texts[i]
            reference = target_texts[i]

            # Generate the prompt using prompt_dict
            prompt = construct_prompt(text)

            # Tokenize the prompt and move it to the same device as the model
            inputs = tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                padding="max_length",
                max_length=128
            )
            inputs = {key: tensor.to(device) for key, tensor in inputs.items()}

            # Generate output (translated sentence)
            #outputs = model.generate(inputs["input_ids"], max_new_tokens=100)
            outputs = model.generate(
                inputs["input_ids"],
                attention_mask=inputs["attention_mask"],  # Explicitly include the attention mask
                max_new_tokens=100
          )

            # Decode the output to get the translated text
            translated_text = tokenizer.decode(outputs[0])
            # Extract the translated text
            translated_text = extract_translation(translated_text)
            translated_text = translated_text.replace("<|endoftext|>", "")

            # Compute BLEU score for this single translation
            bleu_score = metric.compute(predictions=[translated_text], references=[[reference]])["score"]

            # Append the individual BLEU score to the list
            bleu_scores.append(bleu_score)

            # Store the translated text and reference for batch BLEU calculation
            translated_texts.append(translated_text)
            references.append(reference)

            # Write reference, target, translated text, and BLEU score to the text file
            file.write(f"Reference: {text}\n")
            file.write(f"Target: {reference}\n")
            file.write(f"Translated: {translated_text}\n")
            file.write(f"BLEU Score: {bleu_score:.2f}\n")
            file.write("-" * 100 + "\n")

    # Calculate the average BLEU score across all sentences
    #average_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0

    # Compute BLEU score for the entire batch (using all the translated and reference sentences)
    batch_bleu = metric.compute(predictions=translated_texts, references=[[ref] for ref in references])["score"]

    #print(f"\nAverage BLEU score: {average_bleu:.2f}")
    print(f"Batch BLEU score: {batch_bleu:.2f}")
    print(f"Detailed results saved to {output_file}")

    # Return both the average BLEU and the batch BLEU score
    return batch_bleu

"""## 3 Model & Tokenizer"""

model_name = "Qwen/Qwen2.5-1.5B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

"""## 4 Data"""

# import the data
uploaded = files.upload()
ecc_df = pd.read_csv("combined_type_id_description.csv")

ecc_df = ecc_df[ecc_df["type"].isin(["Word", "Phrase"])]
#sentence_df = df[df["type"].isin(["Sentence"])]

uploaded = files.upload()
val_df = pd.read_csv("annotation_full_updated.csv")

#annotated_dict_df = annotated_df[["en_description", "zh_description"]]

# Shuffle the DataFrame
ecc_df = ecc_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Compute sizes
total_samples = len(ecc_df)
dev_size = int(0.1 * total_samples)
shot_size = int(0.1 * total_samples)
train_size = int(0.6 * total_samples)
test_size = total_samples - (dev_size + shot_size + train_size)

# Split the DataFrame
dev_df = ecc_df.iloc[:dev_size]
shot_df = ecc_df.iloc[dev_size:dev_size + shot_size]
train_df = ecc_df.iloc[dev_size + shot_size:dev_size + shot_size + train_size]
test_df = ecc_df.iloc[dev_size + shot_size + train_size:]

# Check sizes
print(f"Dev size: {len(dev_df)}, Shot size: {len(shot_df)}, "
      f"Train size: {len(train_df)}, Test size: {len(test_df)}")

dev_data = {
    "source": dev_df["en_description"],
    "target": dev_df["zh_description"]
}

train_data = {
    "source": train_df["en_description"],
    "target": train_df["zh_description"]
}

test_data = {
    "source": test_df["en_description"],
    "target": test_df["zh_description"]
}


val_data = {
    "source": val_df["en_description"],
    "target": val_df["zh_description"],
}

shot_data = {
    "source": shot_df["en_description"],
    "target": shot_df["zh_description"]
}

# Add prompt and target to the one-shot data
demonstrations = []
for source, target in zip(shot_data["source"], shot_data["target"]):
  prompt = construct_prompt(source, one_shot=False)
  prompt += target
  demonstrations.append(prompt)


demonstrations[:3]

# Add prompt to the training data
train_source_prompted = []
train_target_prompted = []
for source, target in zip(train_data["source"], train_data["target"]):
    prompt = construct_prompt(source, one_shot=True)
    train_source_prompted.append(prompt)
    train_target_prompted.append(prompt + target)


print("An example of the training source data:")
print(train_source_prompted[1])
print("-"*90)
print("An example of the training target data:")
print(train_target_prompted[1])

# Convert the dataset to a Hugging Face Dataset
train_dataset = Dataset.from_dict({"source": train_source_prompted, "target": train_target_prompted})

# Tokenize the entire dataset
tokenized_train = train_dataset.map(tokenize, batched=True)

# Add prompt to the dev data and form coresponding target
dev_source_prompted = []
dev_target_prompted = []
for source, target in zip(dev_data["source"], dev_data["target"]):
    prompt = construct_prompt(source, one_shot=True)
    dev_source_prompted.append(prompt)
    dev_target_prompted.append(prompt + target)


print("An example of the training source data:")
print(dev_source_prompted[1])
print("-"*90)
print("An example of the training target data:")
print(dev_target_prompted[1])

# Convert the dataset to a Hugging Face Dataset
dev_dataset = Dataset.from_dict({"source": dev_source_prompted, "target": dev_target_prompted})
tokenized_dev = dev_dataset.map(tokenize, batched=True)

# Convert the dataset to a Hugging Face Dataset
# test and validation dataset will be tokenized during BLEU calculation
test_dataset = Dataset.from_dict({
    "source": test_data["source"].tolist(),
    "target": test_data["target"].tolist()
})

val_dataset = Dataset.from_dict({
    "source": val_data["source"].tolist(),
    "target": val_data["target"].tolist()
})

"""## 5 Training

### Baseline

1. Baseline for testing on training data from ECCParaCorp
"""

print("Qwen Baseline: Evaluating BLEU score without fine tuning")
bleu = compute_bleu_and_save(model, tokenizer, test_dataset, output_file="baseline_ecc_bleu_scores.txt")
print(bleu)

files.download("baseline_ecc_bleu_scores.txt")

"""2. Baseline for testing on CTBD"""

print("Qwen Baseline: Evaluating BLEU score without fine tuning")
bleu = compute_bleu_and_save(model, tokenizer, val_dataset, output_file="baseline_ctbd_bleu_scores.txt")
print(bleu)

files.download('baseline_ctbd_bleu_scores.txt')

"""### Fine-tuning"""

model_name = "Qwen/Qwen2.5-1.5B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Training arguments
training_args = TrainingArguments(
    output_dir="./qwen-exam",
    eval_strategy="epoch", # Updated from evaluation_strategy
    learning_rate=1e-4, #1e-4, 2e-4, 3e-4
    per_device_train_batch_size=16, # 8, 16, 32
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    warmup_steps=4000,
    save_total_limit=3,
    num_train_epochs=8,
    #predict_with_generate=True,
    logging_dir="./logs",
    logging_steps=40,
    save_strategy="epoch",
    remove_unused_columns=False
)

# Create Trainer instance and include the callback
trainer = Trainer(
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_dev,
    model=model,
    #data_collator=data_collator
    #callbacks=[early_stopping_callback]
)

torch.cuda.empty_cache()

# Fine-tune the model
print("Starting fine-tuning...")
trainer.train()

"""## 6 Testing

### Test on training data from ECCParaCorp
"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

print("Qwen: Evaluating BLEU score with part of ECCParaCorp")
bleu = compute_bleu_and_save(model, tokenizer, test_dataset, output_file="test_ecc_1-16-8.txt")
print(bleu)

files.download('test_ecc_1-16-8.txt')

"""### Test on CTBD"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

print("Qwen: Evaluating BLEU score with manually annotated corpus")
bleu = compute_bleu_and_save(model, tokenizer, val_dataset, output_file="val_annotate_1-16-8.txt")
print(bleu)

"""## 7 Save the trained model"""

from google.colab import drive
drive.mount('/content/drive')

output_dir = "/content/drive/My Drive/qwen-cancer-finetuned_term-1-16-8"

# Save the fine-tuned model
model.save_pretrained(output_dir)

# Save the tokenizer
tokenizer.save_pretrained(output_dir)

print(f"Fine-tuned model and tokenizer saved to {output_dir}")

# Zip the model directory
!zip -r qwen-cancer-finetuned_term-1-16-8.zip "/content/drive/My Drive/qwen-cancer-finetuned_term-1-16-8"

# Download the zipped model
files.download("qwen-cancer-finetuned_term-1-16-8.zip")